{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from einops import rearrange, repeat\n",
    "from image_tools.sizes import resize_and_crop\n",
    "import math\n",
    "import torch\n",
    "import kornia as K\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from plantcv import plantcv as pcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install plantcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import leaf photos\n",
    "folder = r'/home/exouser/ME592_Spring2023/Assignment2/AgandBio/leaves'\n",
    "\n",
    "new_photos = r'/home/exouser/ME592_Spring2023/Assignment2/AgandBio/AugmentedLeaves'\n",
    "\n",
    "list_files = os.listdir(folder)\n",
    "image_paths = []\n",
    "#Loop through photos\n",
    "for file in list_files:\n",
    "    full_path = os.path.join(folder, file)\n",
    "    print(full_path)\n",
    "    image_paths.append(full_path) \n",
    "    \n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_photos = []\n",
    "for x in range(0,100):\n",
    "    # Randomly select image\n",
    "    idx = np.random.randint(0, len(image_paths))\n",
    "    #Randomly mutate\n",
    "    mut = np.random.randint(0,4)\n",
    "    img = cv2.imread(image_paths[idx])\n",
    "    imgnum = str(x) + \".png\"\n",
    "    new_path = os.path.join(new_photos, imgnum)\n",
    "    aug_photos.append(new_path)\n",
    "    print(img.shape)\n",
    "    print(new_path)\n",
    "    if mut == 0:\n",
    "        print(idx, \"rotate\")\n",
    "        angle = np.random.choice([90,180])\n",
    "        if angle == 90:\n",
    "            temp = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif angle == 180:\n",
    "            temp = cv2.rotate(img, cv2.ROTATE_180)\n",
    "        cv2.imwrite(new_path, temp)\n",
    "    elif mut == 1:\n",
    "        print(idx, \"shift\")\n",
    "        M = np.float32([\n",
    "            [1,0,np.random.randint(-95,95)],\n",
    "            [0,1,np.random.randint(-85,85)]\n",
    "        ])\n",
    "        temp = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "        cv2.imwrite(new_path, temp)\n",
    "    elif mut == 2:\n",
    "        print(idx, \"scale\")\n",
    "        # Define the scale range\n",
    "        scale_range = (0.5, 2.0)\n",
    "        # Generate a random scale factor\n",
    "        scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "        # Scale the image\n",
    "        temp = cv2.resize(img, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n",
    "        cv2.imwrite(new_path, temp)\n",
    "    elif mut == 3:\n",
    "        print(idx, \"warp\")\n",
    "        # Define the source and destination points for the warp\n",
    "        src_points = np.float32([[img.shape[0],0],\n",
    "                                 [img.shape[0],img.shape[1]], \n",
    "                                 [0,0],\n",
    "                                 [0,img.shape[1]]\n",
    "                                ])\n",
    "        dst_points = np.float32([[img.shape[0] - np.random.randint(0,95),0 + np.random.randint(0,85)],\n",
    "                                 [img.shape[0] - np.random.randint(0,95),img.shape[1] - np.random.randint(0,85)], \n",
    "                                 [0 + np.random.randint(0,95),0 + np.random.randint(0,85)],\n",
    "                                 [0 + np.random.randint(0,95),img.shape[1] - np.random.randint(0,85)]\n",
    "                                ])\n",
    "        # Calculate the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "        # Apply the perspective transform to the image\n",
    "        temp = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))\n",
    "        cv2.imwrite(new_path, temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size):\n",
    "    b, c, H, W = x.shape\n",
    "    x = rearrange(x, 'b c (h nph ph) (w npw pw) -> b (h nph) (w npw) ph pw c', nph=H//patch_size, npw=W//patch_size, ph=patch_size, pw=patch_size)\n",
    "    x = rearrange(x, 'b (h nph) (w npw) ph pw c -> b (h nph w npw) ph pw c', nph=H//patch_size, npw=W//patch_size)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch_folder = r'/home/exouser/ME592_Spring2023/Assignment2/AgandBio/Patches'\n",
    "list_o_patches = []\n",
    "for x in range(0,len(aug_photos)):\n",
    "    img_to_resize = resize_and_crop(aug_photos[x], (400,350), crop_origin=\"middle\")\n",
    "    print(aug_photos[x])\n",
    "    imgnum = 'Patchesof_' + str(x) + \".npy\"\n",
    "    new_path = os.path.join(patch_folder, imgnum)\n",
    "    list_o_patches.append(new_path)\n",
    "    output = img_to_resize.save('temp.png')\n",
    "    img = cv2.imread('temp.png')\n",
    "    pic = rearrange(img, 'h w c -> 1 h w c')\n",
    "    patches = img_to_patch(rearrange(pic, 'b w h c -> b c h w'),50)\n",
    "    print(new_path)\n",
    "    np.save(new_path, patches)\n",
    "    print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patches(patches):\n",
    "    n_patches = len(patches)\n",
    "    n_cols = int(math.sqrt(n_patches))\n",
    "    n_rows = math.ceil(n_patches/n_cols)\n",
    "    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20,20))\n",
    "    axs = axs.reshape(-1)\n",
    "    for i, patch in enumerate(patches):\n",
    "        axs[i].imshow(patch)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load(r'/home/exouser/ME592_Spring2023/Assignment2/AgandBio/Patches/Patchesof_0.npy')\n",
    "#plot_patches(temp.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the whole dataset\n",
    "tensor_folder = r'/home/exouser/ME592_Spring2023/Assignment2/AgandBio/Tensors'\n",
    "list_o_tensors = []\n",
    "\n",
    "for i in range(len(list_o_patches)):\n",
    "    images = []\n",
    "    temp = np.load(list_o_patches[i])\n",
    "    temp = rearrange(temp, 'b p h w c -> (b p) h w c')\n",
    "    imgnum = 'Tensorof_' + str(i) + \".pt\"\n",
    "    new_path = os.path.join(tensor_folder, imgnum)\n",
    "    list_o_tensors.append(new_path)\n",
    "    for a in range(0, temp.shape[0]):\n",
    "        tempds = temp[a]\n",
    "        torch_temp = torch.from_numpy(tempds)\n",
    "        images.append(torch_temp)\n",
    "    images_torch = torch.stack(images, dim = 0).to(device)\n",
    "    print(images_torch.shape)\n",
    "    zca = K.enhance.ZCAWhitening(eps=0.1)\n",
    "    images_zca = zca(images_torch.float(), include_fit = True)\n",
    "    torch.save(images_zca, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load(list_o_tensors[0])\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load(r'/home/exouser/ME592_Spring2023/Assignment2/AgandBio/Tensors/Tensorof_0.pt')\n",
    "temp = temp.cpu().numpy()\n",
    "plot_patches(temp.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white_patch_folder = r'/home/exouser/ME592_Spring2023/Assignment2/AgandBio/WhitePatches'\n",
    "# list_o_white_patches = []\n",
    "\n",
    "# for i in range(0,len(list_o_tensors)):\n",
    "#     temp = torch.load(list_o_tensors[i])\n",
    "#     print(list_o_tensors[i])\n",
    "#     print(temp.shape[0])\n",
    "#     for x in range(0,temp.shape[0]):\n",
    "#         temp_tensor_patch = temp[x]\n",
    "#         temp_tensor_patch = rearrange(temp_tensor_patch, 'h w c -> c h w')\n",
    "#         print(temp_tensor_patch.shape)\n",
    "#         image = Image.fromarray(temp_tensor_patch.permute(1, 2, 0).cpu().numpy().astype('uint8'))\n",
    "#         file_name = \"WhitePatchof_\" + str(i) + '_' + str(x) + '.jpg'\n",
    "#         list_o_white_patches.append(os.path.join(white_patch_folder, file_name))\n",
    "#         image.save(os.path.join(white_patch_folder, file_name))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the images\n",
    "folder_path = \"/home/exouser/ME592_Spring2023/Assignment2/AgandBio/AugmentedLeaves\"\n",
    "\n",
    "# List the image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith(\".jpg\") or f.endswith(\".jpeg\")]\n",
    "\n",
    "# Initialize a list to hold the amounts of red, green, and blue in each image\n",
    "red_amounts = []\n",
    "green_amounts = []\n",
    "blue_amounts = []\n",
    "\n",
    "# Loop over the images and calculate the amounts of red, green, and blue in each\n",
    "for file_name in image_files:\n",
    "    # Open the image and convert it to RGB\n",
    "    img = Image.open(os.path.join(folder_path, file_name)).convert(\"RGB\")\n",
    "    # Calculate the amounts of red, green, and blue in the image\n",
    "    red_amount = sum([pixel[0] for pixel in img.getdata()])\n",
    "    green_amount = sum([pixel[1] for pixel in img.getdata()])\n",
    "    blue_amount = sum([pixel[2] for pixel in img.getdata()])\n",
    "    # Append the amounts to the lists\n",
    "    red_amounts.append(red_amount)\n",
    "    green_amounts.append(green_amount)\n",
    "    blue_amounts.append(blue_amount)\n",
    "\n",
    "# Create a stacked bar plot of the amounts of red, green, and blue\n",
    "plt.bar(range(len(image_files)), red_amounts, color=\"red\", label=\"Red\")\n",
    "plt.bar(range(len(image_files)), green_amounts, bottom=red_amounts, color=\"green\", label=\"Green\")\n",
    "plt.bar(range(len(image_files)), blue_amounts, bottom=[sum(x) for x in zip(red_amounts, green_amounts)], color=\"blue\", label=\"Blue\")\n",
    "#plt.xticks(range(len(image_files)), image_files, rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Image\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty lists to store the RGB values\n",
    "red_vals = []\n",
    "green_vals = []\n",
    "blue_vals = []\n",
    "\n",
    "for i in range(0,len(list_o_tensors)):\n",
    "    temp = torch.load(list_o_tensors[i])\n",
    "    print(list_o_tensors[i])\n",
    "    #print(temp.shape[0])\n",
    "    for x in range(0,temp.shape[0]):\n",
    "        temp_tensor_patch = temp[x]\n",
    "        image_array = temp_tensor_patch.cpu().numpy()\n",
    "        # extract each channel from the array\n",
    "        red_channel = image_array[:, :, 0]\n",
    "        green_channel = image_array[:, :, 1]\n",
    "        blue_channel = image_array[:, :, 2]\n",
    "        red_vals.append(sum(sum(red_channel)))\n",
    "        green_vals.append(sum(sum(green_channel)))\n",
    "        blue_vals.append(sum(sum(blue_channel)))\n",
    "\n",
    "# Create a stacked bar plot of the amounts of red, green, and blue\n",
    "plt.bar(range(len(red_vals)), red_vals, color=\"red\", label=\"Red\")\n",
    "plt.bar(range(len(red_vals)), green_vals, bottom=red_vals, color=\"green\", label=\"Green\")\n",
    "plt.bar(range(len(red_vals)), blue_vals, bottom=[sum(x) for x in zip(red_vals, green_vals)], color=\"blue\", label=\"Blue\")\n",
    "#plt.xticks(range(len(image_files)), image_files, rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Image\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start on question 2: Image segmentation#\n",
    "#Approach 1: Using a tutorial from PlantCV and selecting color subchannels#\n",
    "#Tutorial: https://plantcv.readthedocs.io/en/stable/tutorials/watershed_segmentation_tutorial/\n",
    "class options:\n",
    "    def __init__(self):\n",
    "        self.image = \"/home/exouser/ME592_Spring2023/Homework/data/Arabidopsis/1.jpg\"\n",
    "        self.debug = \"plot\"\n",
    "        self.writeimg= False \n",
    "        self.outdir = \".\"\n",
    "# Get options\n",
    "args = options()\n",
    "\n",
    "# Set debug to the global parameter \n",
    "pcv.params.debug = args.debug\n",
    "\n",
    "# Read image\n",
    "\n",
    "# Inputs:\n",
    "#   filename - Image file to be read in \n",
    "#   mode - Return mode of image; either 'native' (default), 'rgb', 'gray', or 'csv' \n",
    "\n",
    "img, path, filename = pcv.readimage(filename=args.image)\n",
    "\n",
    "\n",
    "# Convert image from RGB color space to LAB and keep only the \n",
    "# lightness channel \n",
    "\n",
    "# Inputs:\n",
    "#    rgb_img = image object, RGB color space\n",
    "#    channel = color subchannel ('l' = lightness, 'a' = green-magenta , 'b' = blue-yellow)\n",
    "\n",
    "a = pcv.rgb2gray_lab(rgb_img=img, channel='l')\n",
    "\n",
    "# Set a binary threshold on the image \n",
    "\n",
    "# Inputs:\n",
    "#    gray_img    = img object, grayscale\n",
    "#    threshold   = threshold value (0-255)\n",
    "#    max_value   = value to apply above threshold (usually 255 = white)\n",
    "#    object_type = light or dark\n",
    "#       - If object is light then standard thresholding is done\n",
    "#       - If object is dark then inverse thresholding is done\n",
    "\n",
    "img_binary = pcv.threshold.binary(gray_img=a, threshold=40, max_value=255, object_type='light')\n",
    "\n",
    "# Find objects\n",
    "\n",
    "# Inputs:\n",
    "#    img  = image that the objects will be overlayed\n",
    "#    mask = what is used for object detection\n",
    "\n",
    "id_objects, obj_hierarchy = pcv.find_objects(img=img, mask=img_binary)\n",
    "\n",
    "# Combine objects \n",
    "\n",
    "# Inputs:\n",
    "#   img       = RGB or grayscale image data for plotting \n",
    "#   contours  = Contour list \n",
    "#   hierarchy = Contour hierarchy array \n",
    "\n",
    "obj, mask = pcv.object_composition(img=img, contours=id_objects, hierarchy=obj_hierarchy)\n",
    "\n",
    "# Appy mask\n",
    "\n",
    "# Inputs:\n",
    "#   img        = RGB or grayscale image data \n",
    "#   mask       = Binary mask image data \n",
    "#   mask_color = 'white' or 'black' \n",
    "\n",
    "masked = pcv.apply_mask(img=img, mask=mask, mask_color=\"black\")\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(masked)\n",
    "\n",
    "img2 = masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start on question 2: Image segmentation#\n",
    "#Approach 2: Using a different PlantCV tutorial and selecting saturation subchannels#\n",
    "#Tutorial: https://plantcv.readthedocs.io/en/latest/tutorials/vis_tutorial/\n",
    "class options:\n",
    "    def __init__(self):\n",
    "        self.image = \"/home/exouser/ME592_Spring2023/Homework/data/Arabidopsis/1.jpg\"\n",
    "        self.debug = \"plot\"\n",
    "        self.writeimg= False \n",
    "        self.outdir = \".\"\n",
    "# Get options\n",
    "args = options()\n",
    "\n",
    "# Set debug to the global parameter \n",
    "pcv.params.debug = args.debug\n",
    "\n",
    "# Read image\n",
    "\n",
    "# Inputs:\n",
    "#   filename - Image file to be read in \n",
    "#   mode - Return mode of image; either 'native' (default), 'rgb', 'gray', or 'csv' \n",
    "\n",
    "img, path, filename = pcv.readimage(filename=args.image)\n",
    "\n",
    "# Convert RGB to HSV and extract the saturation channel\n",
    "\n",
    "# Inputs:\n",
    "#   rgb_image - RGB image data \n",
    "#   channel - Split by 'h' (hue), 's' (saturation), or 'v' (value) channel\n",
    "s = pcv.rgb2gray_hsv(rgb_img=img, channel='s')\n",
    "\n",
    "# Take a binary threshold to separate plant from background. \n",
    "# Threshold can be on either light or dark objects in the image. \n",
    "\n",
    "# Inputs:\n",
    "#   gray_img - Grayscale image data \n",
    "#   threshold- Threshold value (between 0-255)\n",
    "#   max_value - Value to apply above threshold (255 = white) \n",
    "#   object_type - 'light' (default) or 'dark'. If the object is lighter than \n",
    "#                 the background then standard threshold is done. If the object \n",
    "#                 is darker than the background then inverse thresholding is done. \n",
    "s_thresh = pcv.threshold.binary(gray_img=s, threshold=150, max_value=255, object_type='dark')\n",
    "\n",
    "\n",
    "# Median Blur to clean noise \n",
    "\n",
    "# Inputs: \n",
    "#   gray_img - Grayscale image data \n",
    "#   ksize - Kernel size (integer or tuple), (ksize, ksize) box if integer input,\n",
    "#           (n, m) box if tuple input \n",
    "s_mblur = pcv.median_blur(gray_img=s_thresh, ksize=2) #Lower ksize means less blurring, so more similar to previous image step#\n",
    "\n",
    "\n",
    "# Convert RGB to LAB and extract the lightness channel ('l')\n",
    "\n",
    "# Input:\n",
    "#   rgb_img - RGB image data \n",
    "#   channel- Split by 'l' (lightness), 'a' (green-magenta), or 'b' (blue-yellow) channel\n",
    "b = pcv.rgb2gray_lab(rgb_img=img, channel='l')\n",
    "\n",
    "# Threshold the blue channel image \n",
    "b_thresh = pcv.threshold.binary(gray_img=b, threshold=160, max_value=255, \n",
    "                                object_type='light')\n",
    "\n",
    "\n",
    "# Join the threshold saturation and lightness images with a logical or operation \n",
    "\n",
    "# Inputs: \n",
    "#   bin_img1 - Binary image data to be compared to bin_img2\n",
    "#   bin_img2 - Binary image data to be compared to bin_img1\n",
    "bs = pcv.logical_or(bin_img1=s_mblur, bin_img2=b_thresh)\n",
    "\n",
    "# Appy Mask\n",
    "\n",
    "# Inputs:\n",
    "#   img - RGB or grayscale image data \n",
    "#   mask - Binary mask image data \n",
    "#   mask_color - 'white' or 'black' \n",
    "masked = pcv.apply_mask(img=img, mask=bs, mask_color='white')\n",
    "\n",
    "plt.figure(figsize=(8, 8)) #Use so that the displayed image is large enough to view details#\n",
    "plt.axis('off') #Remove the tickmarks on the axis#\n",
    "plt.imshow(masked) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 3: Threshold off of background color#\n",
    "#Code adapted from https://stackoverflow.com/questions/64491530/how-to-remove-the-background-from-a-picture-in-opencv-python\n",
    "\n",
    "img = cv2.imread(\"/home/exouser/ME592_Spring2023/Assignment2/AgandBio/1.jpg\")\n",
    "\n",
    "hh, ww = img.shape[:2]\n",
    "\n",
    "# threshold on background color\n",
    "# Define lower and uppper limits\n",
    "lower = np.array([0, 0, 255])\n",
    "upper = np.array([115, 150, 255])\n",
    "\n",
    "# Create mask to only select black\n",
    "thresh = cv2.inRange(img, lower, upper)\n",
    "\n",
    "# apply morphology\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))\n",
    "morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# invert morp image\n",
    "mask = 255 - morph\n",
    "\n",
    "# apply mask to image\n",
    "result = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "# save results\n",
    "cv2.imwrite('Q2_thresh.jpg', thresh)\n",
    "cv2.imwrite('Q2_morph.jpg', morph)\n",
    "cv2.imwrite('Q2_mask.jpg', mask)\n",
    "cv2.imwrite('Q2_result.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 4: Using OpenCV to detect and remove grid lines in the background. Mainly will get the horizontal lines#\n",
    "#Found this code and modified some from a person trying to remove music scales from the notes#\n",
    "#https://stackoverflow.com/questions/46274961/removing-horizontal-lines-in-image-opencv-python-matplotlib\n",
    "\n",
    "image = cv2.imread('/home/exouser/ME592_Spring2023/Homework/data/Arabidopsis/1.jpg')\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "thresh = cv2.threshold(gray, 250, 100, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# Remove horizontal\n",
    "horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25,1))\n",
    "detected_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
    "cnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "for c in cnts:\n",
    "    cv2.drawContours(image, [c], -1, (255,255,255), 2)\n",
    "\n",
    "# Repair image\n",
    "repair_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,6))\n",
    "result = 255 - cv2.morphologyEx(255 - image, cv2.MORPH_CLOSE, repair_kernel, iterations=1)\n",
    "\n",
    "cv2.imshow('thresh', thresh)\n",
    "cv2.imshow('detected_lines', detected_lines)\n",
    "cv2.imshow('image', image)\n",
    "cv2.imshow('result', result)\n",
    "\n",
    "#plt.figure(figsize=(8, 8))\n",
    "#plt.imshow(img)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(thresh)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(image)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
